#!/usr/bin/env python3
"""
Setup script for Web Scraping version of Notion Reddit Analysis Tool
No API credentials required!
"""

import os
import sys
from pathlib import Path

def create_directories():
    """Create necessary project directories"""
    directories = [
        'data',
        'models', 
        'reports',
        'visualizations'
    ]
    
    print("ðŸ“ Creating project directories...")
    for directory in directories:
        Path(directory).mkdir(exist_ok=True)
        print(f"  âœ… {directory}/")

def check_requirements():
    """Check if scraping requirements are installed"""
    print("\nðŸ“‹ Checking scraping requirements...")
    
    required_packages = [
        ('requests', 'requests'),
        ('bs4', 'beautifulsoup4'),
        ('lxml', 'lxml'),
        ('fake_useragent', 'fake-useragent'),
        ('pandas', 'pandas'), 
        ('scikit-learn', 'sklearn'),
        ('matplotlib', 'matplotlib'),
        ('seaborn', 'seaborn'),
        ('wordcloud', 'wordcloud'),
        ('textblob', 'textblob')
    ]
    
    missing_packages = []
    
    for import_name, package_name in required_packages:
        try:
            __import__(import_name)
            print(f"  âœ… {package_name}")
        except ImportError:
            print(f"  âŒ {package_name}")
            missing_packages.append(package_name)
    
    if missing_packages:
        print(f"\nâš ï¸  Missing packages: {', '.join(missing_packages)}")
        print("Run: pip install -r requirements_scraper.txt")
        return False
    
    print("\nâœ… All requirements satisfied!")
    return True

def test_internet_connection():
    """Test if we can reach Reddit"""
    print("\nðŸŒ Testing internet connection to Reddit...")
    
    try:
        import requests
        response = requests.get('https://www.reddit.com/r/test.json', timeout=10)
        
        if response.status_code == 200:
            print("  âœ… Can reach Reddit successfully")
            return True
        else:
            print(f"  âš ï¸  Reddit returned status code: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"  âŒ Cannot reach Reddit: {e}")
        print("  ðŸ”§ Check your internet connection")
        return False

def test_scraper():
    """Test the web scraper functionality"""
    print("\nðŸ§ª Testing web scraper...")
    
    try:
        from reddit_scraper import RedditWebScraper
        
        scraper = RedditWebScraper()
        
        # Test JSON API method with very small request
        posts = scraper.scrape_subreddit_json('Notion', 'hot', 3)
        
        if posts and len(posts) > 0:
            print(f"  âœ… Scraper working! Found {len(posts)} test posts")
            print(f"  ðŸ“‹ Sample: '{posts[0]['title'][:50]}...'")
            return True
        else:
            print("  âš ï¸  Scraper returned no posts")
            print("  ðŸ”§ This might be temporary - try running again")
            return False
            
    except Exception as e:
        print(f"  âŒ Scraper test failed: {e}")
        return False

def main():
    """Main setup function for web scraping version"""
    print("ðŸ•·ï¸ Setting up Notion Reddit Web Scraping Tool")
    print("=" * 55)
    print("âœ¨ NO API CREDENTIALS REQUIRED!")
    print("ðŸŒ Uses public Reddit endpoints")
    
    # Create directories
    create_directories()
    
    # Check requirements
    if not check_requirements():
        print("\nâŒ Setup incomplete - please install missing requirements")
        print("Run: pip install -r requirements_scraper.txt")
        return False
    
    # Test internet connection
    internet_ok = test_internet_connection()
    
    # Test scraper if internet is working
    scraper_ok = False
    if internet_ok:
        scraper_ok = test_scraper()
    
    print("\n" + "=" * 55)
    
    if internet_ok and scraper_ok:
        print("ðŸŽ‰ Setup complete! Web scraper is ready!")
        print("\nðŸš€ Next steps:")
        print("  ðŸ§ª Test scraper: python main_pipeline_scraper.py test")
        print("  âš¡ Quick start: python main_pipeline_scraper.py quick")
        print("  ðŸ“Š Full analysis: python main_pipeline_scraper.py full")
        print("  ðŸ“ˆ Dashboard: streamlit run dashboard.py")
        
        print("\nðŸ’¡ Advantages of web scraping:")
        print("  â€¢ No API credentials needed")
        print("  â€¢ No rate limiting issues")
        print("  â€¢ Access to public Reddit data")
        print("  â€¢ Works immediately after setup")
        
        return True
    else:
        print("âš ï¸ Setup completed with warnings")
        
        if not internet_ok:
            print("  ðŸŒ Internet connection issue - check network")
        if internet_ok and not scraper_ok:
            print("  ðŸ•·ï¸ Scraper test failed - might be temporary")
            print("  ðŸ”„ Try running: python main_pipeline_scraper.py test")
        
        print("\nðŸ“‹ You can still proceed:")
        print("  python main_pipeline_scraper.py test")
        return True

def show_comparison():
    """Show comparison between API and scraping approaches"""
    print("\nðŸ“Š API vs Web Scraping Comparison:")
    print("=" * 40)
    
    print("ðŸ”‘ PRAW (API) Approach:")
    print("  âœ… More stable data structure")
    print("  âœ… Official Reddit support")  
    print("  âŒ Requires API credentials")
    print("  âŒ Rate limiting (60 requests/min)")
    print("  âŒ Setup complexity")
    
    print("\nðŸ•·ï¸ Web Scraping Approach:")
    print("  âœ… No credentials required")
    print("  âœ… Immediate setup")
    print("  âœ… Higher rate limits")
    print("  âœ… Access to public data")
    print("  âŒ May break if Reddit changes HTML")
    print("  âŒ Potential IP blocking")
    
    print("\nðŸ’¡ Recommendation:")
    print("  Start with web scraping for quick results")
    print("  Switch to API for production use")

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1].lower() == 'compare':
        show_comparison()
    else:
        success = main()
        sys.exit(0 if success else 1)